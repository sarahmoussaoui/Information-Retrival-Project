Data folder layout and usage
============================

Raw data (data/raw)
-------------------
- MED.ALL   : 1,033 documents (indexed)
- MED.QRY   : 30 queries (not indexed, but preprocessed the same way as docs)
- MED.REL   : relevance judgments (ground truth)
- MED.REL.OLD : legacy/ignored

Processed outputs (data/processed)
----------------------------------
- parse_preprocess/
	- docs_processed.json      : {doc_id: [tokens]}
	- queries_processed.json   : {query_id: [tokens]}
	- qrels.json               : {query_id: [doc_id, ...]} (lists for JSON compatibility)

- build_tf_idf_stats/
	- n_docs.json                 : {"n_docs": int}
	- doc_tf.json                 : {doc_id: {term: tf}}
	- doc_tf_max.json             : {doc_id: tf_max}
	- doc_tf_norm.json            : {doc_id: {term: tf_norm}}           # tf_norm = tf / tf_max per doc
	- doc_freq.json               : {term: df}
	- collection_tf.json          : {term: total_tf_across_all_docs}
	- collection_tf_norm.json     : {term: tf_total / max(tf_total)}

- build_index/
	- doc_term_matrix.npz      : CSR matrix (rows = docs, cols = vocab, values = term frequency)
	- doc_index.json           : {doc_id: row_index}
	- doc_lengths.json         : {doc_id: token_count}
	- vocab.json               : {term: col_index}
	- inverted_index.json      : {term: [(doc_id, tf-idf), ...]} (postings sorted by doc_id)
	- doc_term_matrix_binary.npz : CSR matrix with 0/1 term presence (rows = docs, cols = vocab)
	- avg_doc_length.json      : {"avg_doc_length": mean_token_count}

How to regenerate
-----------------
- Run the Windows pipeline: scripts\run_pipeline.bat
- Or stepwise:
	1) python scripts\downloadingToProcessing\test_parser.py
	2) python scripts\downloadingToProcessing\build_tf_idf_stats.py
	3) python scripts\downloadingToProcessing\build_index.py
